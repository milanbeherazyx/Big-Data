{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Core Components of the Hadoop Ecosystem\n",
    "- **HDFS (Hadoop Distributed File System)**: A distributed file system designed to store large files across multiple machines. It splits files into blocks and stores them across the cluster, ensuring high availability and reliability.\n",
    "  \n",
    "- **MapReduce**: A programming model for processing large datasets in parallel. It breaks the data processing task into two phases: the **Map** phase (where data is transformed into key-value pairs) and the **Reduce** phase (where these pairs are aggregated).\n",
    "\n",
    "- **YARN (Yet Another Resource Negotiator)**: A resource management layer that allocates resources to various applications running in a Hadoop cluster. It improves resource utilization and allows multiple data processing frameworks to run on the same cluster.\n",
    "\n",
    "### 2. Hadoop Distributed File System (HDFS)\n",
    "- **Storage and Management**: HDFS stores data in a distributed manner across nodes in a cluster. Files are divided into blocks (default size is 128 MB) that are distributed across DataNodes, with each block replicated for fault tolerance.\n",
    "\n",
    "- **Key Concepts**:\n",
    "  - **NameNode**: The master server that manages metadata and the namespace of the HDFS. It keeps track of which blocks are stored on which DataNodes.\n",
    "  - **DataNode**: The worker nodes that store the actual data blocks. They communicate with the NameNode to report the status of the blocks they store.\n",
    "  - **Blocks**: The basic unit of storage in HDFS. Each file is split into blocks that are stored across the DataNodes, with a configurable replication factor for reliability.\n",
    "\n",
    "### 3. How MapReduce Works\n",
    "**Step-by-step Explanation**:\n",
    "- **Input Splits**: The input data is divided into manageable chunks.\n",
    "- **Map Phase**: Each chunk is processed by a Mapper, which transforms the input data into key-value pairs.\n",
    "  - *Example*: Counting words in a document. The Mapper outputs pairs like (word, 1).\n",
    "- **Shuffle and Sort**: The framework sorts the key-value pairs by key and groups them together.\n",
    "- **Reduce Phase**: Each Reducer processes the grouped data, aggregating the values for each key.\n",
    "  - *Example*: The Reducer sums the counts for each word.\n",
    "\n",
    "**Advantages and Limitations**:\n",
    "- **Advantages**: Scalability, fault tolerance, and high throughput for batch processing.\n",
    "- **Limitations**: High latency, complexity in debugging, and difficulty in iterative processing.\n",
    "\n",
    "### 4. Role of YARN in Hadoop\n",
    "- **Resource Management**: YARN allocates resources dynamically to various applications, improving resource utilization compared to the earlier Hadoop 1.x model.\n",
    "  \n",
    "- **Comparison with Hadoop 1.x**:\n",
    "  - In Hadoop 1.x, the JobTracker managed both resource allocation and job scheduling, leading to bottlenecks. YARN separates these functions, allowing multiple applications to run simultaneously without resource conflicts.\n",
    "\n",
    "### 5. Popular Components within the Hadoop Ecosystem\n",
    "- **HBase**: A NoSQL database that provides real-time read/write access to large datasets. Use case: Storing user profiles for real-time web applications.\n",
    "  \n",
    "- **Hive**: A data warehouse software that facilitates querying and managing large datasets using SQL-like language (HiveQL). Use case: Data analysis and reporting.\n",
    "  \n",
    "- **Pig**: A high-level platform for creating programs that run on Hadoop, using a language called Pig Latin. Use case: ETL processes.\n",
    "  \n",
    "- **Spark**: A fast, in-memory data processing engine that supports batch and streaming data. Use case: Real-time analytics and machine learning.\n",
    "\n",
    "**Integration Example**: Spark can be integrated into a Hadoop ecosystem for real-time data processing tasks, enabling faster computations compared to MapReduce.\n",
    "\n",
    "### 6. Differences Between Apache Spark and Hadoop MapReduce\n",
    "- **In-Memory Processing**: Spark processes data in memory, which is significantly faster than MapReduce's disk-based processing.\n",
    "  \n",
    "- **Ease of Use**: Spark offers high-level APIs in multiple languages (Python, Scala, Java), whereas MapReduce requires more boilerplate code.\n",
    "\n",
    "- **Support for Streaming and Batch Processing**: Spark natively supports both types of data processing, while MapReduce is primarily designed for batch jobs.\n",
    "\n",
    "### 7. Spark Application Example (Word Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize SparkContext\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord Count\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Read the input text file\u001b[39;00m\n\u001b[0;32m      7\u001b[0m text_file \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pyspark\\java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = SparkContext(\"local\", \"Word Count\")\n",
    "\n",
    "# Read the input text file\n",
    "text_file = sc.textFile(\"input.txt\")\n",
    "\n",
    "# Count the occurrences of each word\n",
    "word_counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "                        .map(lambda word: (word, 1)) \\\n",
    "                        .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Get the top 10 most frequent words\n",
    "top_10 = word_counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "# Print the result\n",
    "for word, count in top_10:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Stop SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Spark RDD Tasks\n",
    "Assuming you have an RDD called `dataRDD`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. **Filter**: Selecting rows based on a condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataRDD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m dataRDD\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m row: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumn_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m threshold)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataRDD' is not defined"
     ]
    }
   ],
   "source": [
    "filtered_data = dataRDD.filter(lambda row: row['column_name'] > threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. **Map**: Modifying a specific column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_data = filtered_data.map(lambda row: row['column_to_modify'] * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. **Reduce**: Calculating an aggregation (e.g., average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_value = mapped_data.reduce(lambda a, b: a + b) / mapped_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Creating a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataFrame Example\").getOrCreate()\n",
    "\n",
    "# Load a CSV file into a DataFrame\n",
    "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# a. Select specific columns\n",
    "selected_df = df.select(\"column1\", \"column2\")\n",
    "\n",
    "# b. Filter rows\n",
    "filtered_df = df.filter(df.column_name > threshold)\n",
    "\n",
    "# c. Group by a column and calculate aggregations\n",
    "grouped_df = df.groupBy(\"group_column\").agg({\"agg_column\": \"sum\"})\n",
    "\n",
    "# d. Join two DataFrames\n",
    "joined_df = df1.join(df2, on=\"common_key\", how=\"inner\")\n",
    "\n",
    "# Show the result\n",
    "joined_df.show()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 10. Setting Up a Spark Streaming Application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Initialize SparkContext and StreamingContext\n",
    "sc = SparkContext(\"local[2]\", \"Streaming Example\")\n",
    "ssc = StreamingContext(sc, 1)  # 1 second batch interval\n",
    "\n",
    "# Create a DStream from a source (e.g., Kafka)\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Apply a transformation (e.g., word count)\n",
    "word_counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "                   .map(lambda word: (word, 1)) \\\n",
    "                   .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Output the results to the console\n",
    "word_counts.pprint()\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 11. Fundamental Concepts of Apache Kafka\n",
    "Apache Kafka is a distributed streaming platform that allows you to publish, subscribe to, and process streams of records in real-time. It solves problems related to real-time data processing, high-throughput data ingestion, and scalability.\n",
    "\n",
    "### 12. Architecture of Kafka\n",
    "- **Producers**: Applications that publish messages to Kafka topics.\n",
    "  \n",
    "- **Topics**: Categories to which records are published. Each topic is divided into partitions.\n",
    "\n",
    "- **Brokers**: Kafka servers that store and manage the data. Each broker handles the requests from producers and consumers.\n",
    "\n",
    "- **Consumers**: Applications that subscribe to topics and process the published messages.\n",
    "\n",
    "- **ZooKeeper**: A centralized service for maintaining configuration information and providing distributed synchronization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Producing and Consuming Data to/from Kafka\n",
    "**Producing Data** (Python example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kafka'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaProducer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize producer\u001b[39;00m\n\u001b[0;32m      4\u001b[0m producer \u001b[38;5;241m=\u001b[39m KafkaProducer(bootstrap_servers\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalhost:9092\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kafka'"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "# Initialize producer\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "\n",
    "# Send data to a topic\n",
    "producer.send('my_topic', b'Hello, Kafka!')\n",
    "\n",
    "# Close producer\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consuming Data** (Python example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kafka'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaConsumer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize consumer\u001b[39;00m\n\u001b[0;32m      4\u001b[0m consumer \u001b[38;5;241m=\u001b[39m KafkaConsumer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_topic\u001b[39m\u001b[38;5;124m'\u001b[39m, bootstrap_servers\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalhost:9092\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kafka'"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "# Initialize consumer\n",
    "consumer = KafkaConsumer('my_topic', bootstrap_servers='localhost:9092')\n",
    "\n",
    "# Consume messages\n",
    "for message in consumer:\n",
    "    print(f\"Received: {message.value}\")\n",
    "\n",
    "# Close consumer\n",
    "consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Importance of Data Retention and Partitioning in Kafka\n",
    "- **Data Retention**: Kafka retains messages for a configurable period, allowing consumers to read messages at their own pace. This is crucial for ensuring data availability.\n",
    "\n",
    "- **Partitioning**: Splitting a topic into partitions allows for parallel processing and improves throughput. Each partition can be consumed by different consumers.\n",
    "\n",
    "### 15. Real-world Use Cases of Apache Kafka\n",
    "- **Log Aggregation**: Collecting logs from multiple sources and centralizing them for processing and monitoring.\n",
    "  \n",
    "- **Real-time Analytics**: Processing streams of data for real-time analytics, such as monitoring user activity on websites.\n",
    "\n",
    "- **Event Sourcing**: Capturing state changes in systems as a series of events for systems like banking or e-commerce.\n",
    "\n",
    "Kafka is preferred in these scenarios due to its high throughput, fault tolerance, scalability, and ability to handle a large volume of real-time data efficiently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
